{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQNdynamic.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNB2WRSmoVUnryno0xeNUGH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":40,"metadata":{"id":"GLgUU_7TXYmY","executionInfo":{"status":"ok","timestamp":1654765977657,"user_tz":-180,"elapsed":2193,"user":{"displayName":"konth konth","userId":"04638664992708773001"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c2ae4e3-feaa-4f7f-af63-bea6ebd89665"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive') # Comment after mounting drive"]},{"cell_type":"code","source":["# After executing the cell above, Drive\n","# files will be present in \"/content/drive/My Drive\". \n","# This cell presents the contents of the supporting files directory\n","# Next cell gets you to the supporting files directory\n","!ls \"/content/drive/MyDrive/ktd1g20_COMP6247/DQNDynamicMaze\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"858xrgfOZUMu","executionInfo":{"status":"ok","timestamp":1654765979187,"user_tz":-180,"elapsed":290,"user":{"displayName":"konth konth","userId":"04638664992708773001"}},"outputId":"cd9c142a-91d3-4ddd-eff8-b546302330e6"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["COMP6247Maze20212022.npy  DQNdynamic.ipynb  __pycache__  read_maze.py\n"]}]},{"cell_type":"code","source":["cd \"/content/drive/MyDrive/ktd1g20_COMP6247/DQNDynamicMaze\" "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlEFyh7jhNkD","executionInfo":{"status":"ok","timestamp":1654765980390,"user_tz":-180,"elapsed":50,"user":{"displayName":"konth konth","userId":"04638664992708773001"}},"outputId":"0c2e9376-7291-4382-c9d3-a40df8a5a362"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ktd1g20_COMP6247/DQNStaticMaze\n"]}]},{"cell_type":"code","execution_count":43,"metadata":{"id":"Kaxo2V7lFuIE","executionInfo":{"status":"ok","timestamp":1654765982165,"user_tz":-180,"elapsed":254,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# The libraries that are needed\n","\n","from __future__ import print_function\n","import os, sys, time, datetime, json, random\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Activation\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n","from keras.layers.advanced_activations import PReLU \n","from tensorflow.keras import initializers\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"_7WyXbN6FuIJ","executionInfo":{"status":"ok","timestamp":1654765982424,"user_tz":-180,"elapsed":4,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# This imports the functions from the read_maze.py file.\n","from read_maze import *"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"sBOsBSz0FuIJ","executionInfo":{"status":"ok","timestamp":1654765982678,"user_tz":-180,"elapsed":4,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# This will load the maze\n","load_maze()"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"Ze0R7HUZFuIK","executionInfo":{"status":"ok","timestamp":1654765985260,"user_tz":-180,"elapsed":2121,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# Just an array to visualise parts of the maze for visualisation purposes.\n","example_array = np.zeros((10,10), dtype=np.float64)\n","for i in range(10):\n","  for j in range(10):\n","    example_array[i][j] = get_local_maze_information(i,j)[1][1][0]\n","    "]},{"cell_type":"code","execution_count":47,"metadata":{"id":"mW2zBMiHFirC","executionInfo":{"status":"ok","timestamp":1654765985261,"user_tz":-180,"elapsed":8,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# We need a function that gives us the state the agent is in.\n","# The function takes as input the coordinates (x,y) of the agent's current location.\n","# The output is a list of 20 elements\n","# The first 9 elements correspond to the walls surrounding the agent\n","# The second 9 elements correspond to the fire surrounding the agent\n","# The last two elements correspond to (x,y) coordinates of the agent's position (similar to the input)\n","\n","def get_state(x,y): \n","  info_array = get_local_maze_information(int(x), int(y))\n","  sides_array = np.zeros((3,3))\n","  fires_array = np.zeros((3,3))\n","  for i in range(3):\n","    for j in range(3):\n","      sides_array[i][j] = info_array[i][j][0] # The spatial geometry\n","      fires_array[i][j] = info_array[i][j][1] # The fire geometry\n","\n","  my_list = sides_array.flatten().tolist() + fires_array.flatten().tolist() \n","  my_list.append(x)\n","  my_list.append(y)\n","  state_array = np.array([my_list])\n","  \n","  return state_array"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"d55WyC-LJbsl","executionInfo":{"status":"ok","timestamp":1654765985558,"user_tz":-180,"elapsed":304,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# This function gets the action from the Q-network.\n","# The inputs are the Q-network and the state the agent is in.\n","# The output is the action the agent should take, represented as a number\n","\n","def get_action_from_Qvalue(model, state):\n","  q = model.predict(state)\n","  action = float(np.argmax(q[0])) \n","\n","  return action "]},{"cell_type":"code","source":["# This function is used to check whether we ended up in a new bad state and updates the set of BadStates accordingly.\n","def newBadState(state):\n","  global BadStates\n","  my_state = state\n","  local_x = my_state[0][-2]\n","  local_y = my_state[0][-1]\n","  if [local_x, local_y] in BadStates: # [x,y] already in BadStates\n","    return False\n","  \n","  # If the sum of cross elements is 1 then we are in a bad state.\n","  cross_sum = my_state[0][1] + my_state[0][3] + my_state[0][5] + my_state[0][7] \n","  if cross_sum <= 1:\n","    # modify the set of BadStates accordingly and report on that by returning True\n","    BadStates.append([local_x,local_y]) \n","    return True \n","  else:\n","    return False "],"metadata":{"id":"3KmTLuND0NcR","executionInfo":{"status":"ok","timestamp":1654765985561,"user_tz":-180,"elapsed":34,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","execution_count":50,"metadata":{"id":"O0CSuUxjRGXK","executionInfo":{"status":"ok","timestamp":1654765985563,"user_tz":-180,"elapsed":32,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# This is the function that enables the agent to move. \n","# The inputs are the state the agent is in and the action it will take based on the Q-table.\n","# The outputs are the new_state, the immdediate reward a Boolean variable \n","# and two Boolean variables: the first one checkes whether the maze has been successfully completed\n","# while the second one records whether the agent should be reset according to the proposed framework adopted.\n","\n","def act(state, action):\n","  game_won = False\n","  temp_action = float(action)\n","  reward = 0\n","  resetAgent = False \n","\n","  start_x = state[0][-2] # Our starting coordinates before the action\n","  start_y = state[0][-1]\n","  old_state = state\n","  isIntersectionMove = [start_x, start_y, float(action)] # This will help to check whether we reached a new intersection\n","\n","  new_x = state[0][-2] # Our coordinates after the action. Initialised as the starting coordinates.\n","  new_y = state[0][-1]\n","  new_state = state\n","\n","  # Check if you have already made this bad intersection move.\n","  if isIntersectionMove in SetBadIntersectionMoves: \n","    reward -= 1000000 # It was -1 initially\n","    new_state = get_state(new_x, new_y)\n","    resetAgent = True\n","    return new_state, reward, game_won, resetAgent\n","\n","  if (temp_action == 0) and (old_state[0][3] == 1) and (old_state[0][12] == 0): # Valid left\n","    new_y -= 1\n","    # Initially check whether the winning tile has been reached.\n","    if new_x == target[0] and new_y == target[1]:\n","      game_won = True\n","      reward += 10000000\n","      new_state = get_state(new_x, new_y)\n","      return new_state, reward, game_won, resetAgent\n","    # Then check whether we have reached a previously discovered bad state\n","    elif [new_x, new_y] in BadStates:\n","      reward -= 1000000\n","      resetAgent = True\n","      new_state = get_state(new_x, new_y)\n","      return new_state , reward, game_won, resetAgent \n","    new_state = get_state(new_x, new_y)\n","    # Then check whether we have reached a new bad state\n","    if newBadState(new_state) == True:\n","      reward -= 1000000\n","      resetAgent = True\n","      return new_state, reward, game_won, resetAgent \n","    # If none of the above is true then continue.\n","    else:\n","      reward -= 100000\n","      # Check if we have already visited this tile\n","      if [new_x, new_y] in visited:\n","        reward += 0\n","      else:\n","        reward += 95000\n","      return new_state, reward, game_won, resetAgent\n","\n","  elif (temp_action == 1) and (old_state[0][1] == 1) and (old_state[0][10] == 0): # Valid up\n","    new_x -= 1\n","    # Initially check whether the winning tile has been reached.\n","    if new_x == target[0] and new_y == target[1]:\n","      game_won = True\n","      reward += 10000000\n","      new_state = get_state(new_x, new_y)\n","      return new_state, reward, game_won, resetAgent\n","    # Then check whether we have reached a previously discovered bad state\n","    elif [new_x, new_y] in BadStates:\n","      reward -= 1000000\n","      resetAgent = True\n","      new_state = get_state(new_x, new_y)\n","      return new_state , reward, game_won, resetAgent \n","    new_state = get_state(new_x, new_y)\n","    # Then check whether we have reached a new bad state\n","    if newBadState(new_state) == True:\n","      reward -= 1000000\n","      resetAgent = True\n","      return new_state, reward, game_won, resetAgent # When reset agent is false update the badstates through the newBadState function and bad intersection moves through the resetAgent list\n","    # If none of the above is true then continue.\n","    else:\n","      reward -= 100000\n","      # Check if we have already visited this tile\n","      if [new_x, new_y] in visited:\n","        reward += 0\n","      else:\n","        reward += 95000\n","      return new_state, reward, game_won, resetAgent\n","\n","  elif (temp_action == 2) and (old_state[0][5] == 1) and (old_state[0][14] == 0): # Valid right\n","    new_y += 1\n","    # Initially check whether the winning tile has been reached.\n","    if new_x == target[0] and new_y == target[1]:\n","      game_won = True\n","      reward += 10000000\n","      new_state = get_state(new_x, new_y)\n","      return new_state, reward, game_won, resetAgent\n","    # Then check whether we have reached a previously discovered bad state\n","    elif [new_x, new_y] in BadStates:\n","      reward -= 1000000\n","      resetAgent = True\n","      new_state = get_state(new_x, new_y)\n","      return new_state, reward, game_won, resetAgent \n","    new_state = get_state(new_x, new_y)\n","    # Then check whether we have reached a new bad state\n","    if newBadState(new_state) == True:\n","      reward -= 1000000\n","      resetAgent = True\n","      return new_state, reward, game_won, resetAgent \n","    # If none of the above is true then continue.\n","    else:\n","      reward -= 100000\n","      # Check if we have already visited this tile\n","      if [new_x, new_y] in visited:\n","        reward += 0\n","      else:\n","        reward += 95000\n","      return new_state, reward, game_won, resetAgent\n","\n","  elif (temp_action == 3) and (old_state[0][7] == 1) and (old_state[0][16] == 0): # Valid down\n","    new_x += 1\n","    # Initially check whether the winning tile has been reached.\n","    if new_x == target[0] and new_y == target[1]:\n","      game_won = True\n","      reward += 10000000\n","      new_state = get_state(new_x, new_y)\n","      return new_state, reward, game_won, resetAgent\n","    # Then check whether we have reached a previously discovered bad state\n","    elif [new_x, new_y] in BadStates:\n","      reward -= 1000000\n","      resetAgent = True\n","      new_state = get_state(new_x, new_y)\n","      return new_state, reward, game_won, resetAgent \n","    new_state = get_state(new_x, new_y)\n","    # Then check whether we have reached a new bad state\n","    if newBadState(new_state) == True:\n","      reward -= 1000000\n","      resetAgent = True\n","      return new_state, reward, game_won, resetAgent # When reset agent is false update the badstates through the newBadState function and bad intersection moves through the resetAgent list\n","    # If none of the above is true then continue.\n","    else:\n","      reward -= 100000\n","      # Check if we have already visited this tile\n","      if [new_x, new_y] in visited:\n","        reward += 0\n","      else:\n","        reward += 95000\n","      return new_state, reward, game_won, resetAgent\n","\n","  elif (temp_action == 4): # Stay put\n","    new_state = get_state(new_x, new_y) # This has to be run so that we can get the fire off.\n","    reward -= 100000 \n","    return new_state, reward, game_won, resetAgent\n","  \n","  else: # Invalid move\n","    reward -= 1000000 # This is the case where the agent tried to walk into a wall or a fire.\n","    return old_state, reward, game_won, resetAgent "]},{"cell_type":"code","execution_count":51,"metadata":{"id":"cfWyzeZ3ZqAb","executionInfo":{"status":"ok","timestamp":1654765985566,"user_tz":-180,"elapsed":32,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# Our neural network model.\n","def build_model(lr=0.001):\n","    lr = lr\n","    model = Sequential()\n","    model.add(Dense(128, kernel_initializer= initializers.RandomUniform(minval=-1, maxval=1, seed=None), input_shape=(20,))) # Our first dense layer accepts the vector of size 20 as input\n","    model.add(PReLU()) # maybe change that\n","    model.add(Dense(128, kernel_initializer= initializers.RandomUniform(minval=-1, maxval=1, seed=None))) \n","    model.add(PReLU())\n","    model.add(Dense(num_actions, kernel_initializer= initializers.RandomUniform(minval=-1, maxval=1, seed=None))) \n","    opt = Adam(learning_rate=lr)\n","    model.compile(optimizer= opt, loss='mse') \n","    return model"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"JUQgDv8L7kx0","executionInfo":{"status":"ok","timestamp":1654765985568,"user_tz":-180,"elapsed":32,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# This will be used to return the output of the neural network:\n","def predict(model, state):\n","  return model.predict(state)[0]"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"oAE4QCTW_JFr","executionInfo":{"status":"ok","timestamp":1654765985844,"user_tz":-180,"elapsed":306,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# This function will be used to sample from our experience buffer and train the network \n","# We must input the desired data size and the memory we have so far as well as the output size of the NN.\n","# We also need the discount factor for this as it is used to copute the expected reward of the next step\n","\n","def get_data( data_size, memory, num_actions, updateModel, targetModel, discount):\n","  state_size = memory[0][0].shape[1] # This is our flattened vector for the local \"image\" of the maze.\n","                                         \n","  mem_size = len(memory) # This is the running length for the memory.\n","  data_size = min (mem_size, data_size) # For the initial steps mem_size < data_size\n","  inputs = np.zeros((data_size, state_size)) # These will serve as our input data to the NN\n","  targets = np.zeros((data_size, num_actions)) # These are the labels to the NN, corresponding to the Q_value per action\n","\n","  # Now we generate our samples with no replacement by pulling #data_size samples from the memory\n","  for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace = False)):\n","    state, action, reward, new_state, game_won = memory[j]\n","    inputs[i] = state # Our input instance\n","    targets[i] = predict(updateModel, state) # This is the current answer we get from the NN\n","\n","    # Use the targetNetwork to predict the Q value \n","    # but first find the argmax of the next state based on the\n","    # updateNetwork\n","    index_Q_sa = np.argmax(predict(updateModel, new_state)) # We use the updateModel to find the next action\n","    Q_sa = predict(targetModel, new_state)[index_Q_sa] # But we evaluate the Q value of the next (state,action) pair based on the targetModel for stability\n","    if game_won:\n","      targets[i, int(action)] = reward\n","    else:\n","      targets[i, int(action)] = reward + discount * Q_sa\n","  return inputs, targets"]},{"cell_type":"code","source":["# This function both checks if a state is an intersection and ,if it indeed is, it adds it to \n","# the listOfIintersections if it was not already there while also checking whether a triplet \n","# of x,y,action is a bad Intersection move.\n","\n","def checkIfIntersection(state):\n","  global listOfIntersections\n","  global BadStates\n","  global SetBadIntersectionMoves \n","  state = state\n","  sum_cross = state[0][1] + state[0][3] + state[0][5] + state[0][7]\n","  if (sum_cross >= 3) and ([state[0][-2], state[0][-1]] not in BadStates):\n","    if [state[0][-2],state[0][-1]] not in listOfIntersections:\n","      listOfIntersections.append([state[0][-2],state[0][-1]])\n","      if (state[0][1] == 0):\n","        SetBadIntersectionMoves.append([state[0][-2],state[0][-1], float(1)])\n","      if (state[0][3] == 0):\n","        SetBadIntersectionMoves.append([state[0][-2],state[0][-1], float(0)])\n","      if (state[0][5] == 0):\n","        SetBadIntersectionMoves.append([state[0][-2],state[0][-1], float(2)])\n","      if (state[0][7] == 0):\n","        SetBadIntersectionMoves.append([state[0][-2],state[0][-1], float(3)])\n","    return True\n","  else:\n","    return False"],"metadata":{"id":"oDHGiKvlJFJA","executionInfo":{"status":"ok","timestamp":1654765986185,"user_tz":-180,"elapsed":9,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Check if an intersection is bad or not\n","\n","def isBadIntersection(x,y): \n","  counter = 0\n","  if [x,y,float(0)] in SetBadIntersectionMoves:\n","     counter += 1\n","  if [x,y,float(1)] in SetBadIntersectionMoves:\n","     counter += 1\n","  if [x,y,float(2)] in SetBadIntersectionMoves:\n","     counter += 1\n","  if [x,y,float(3)] in SetBadIntersectionMoves:\n","     counter += 1\n","  \n","  if (counter >= 3):\n","    return True\n","  else:\n","    return False"],"metadata":{"id":"_WLwtEMFjBoh","executionInfo":{"status":"ok","timestamp":1654765987237,"user_tz":-180,"elapsed":4,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","execution_count":56,"metadata":{"id":"hjUSI0kOK2ey","executionInfo":{"status":"ok","timestamp":1654765987855,"user_tz":-180,"elapsed":269,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# The training function for our dqn implementation\n","def qtrain( epsilon, n_epoch, max_memory, data_size, discount, num_actions, resumingTraining, TrainingFromStart, **opt):\n","  global visited\n","  global lastIntersection\n","  global SetBadIntersectionMoves\n","  global listOfIntersections\n","  global memory\n","  global BadStates\n","  global last_state_coordinates\n","  global actions_dict\n","  global updateModel\n","  global targetModel\n","\n","\n","  bigReset = False # This Boolean value handles reseting the agent in the case of reaching a bad itersection\n","  n_epoch = n_epoch # The number of epochs you wish to train the models\n","\n","  # Auxiliary variables to help with training on Google Colab\n","  resumingTraining = resumingTraining\n","  TrainingFromStart = TrainingFromStart\n","\n","  max_memory = max_memory # The memory of the replay epxerience\n","  data_size = data_size   # The size of the data that are being used to update the models\n","\n","  start_time = datetime.datetime.now() # A counter for time\n","                \n","  discount = discount # The discount factor for delayed reward.\n","  num_actions = num_actions # This is simply the number of available actions at each tile of the maze\n","\n","  # Auxiliary variables for the user interface\n","  random_move = \"\"\n","  model_update_counter = 0\n","  game_over = False\n","\n","  for epoch in range(n_epoch):\n","    if game_over == True:\n","      break\n","    print(\"We are now in epoch #\" , (epoch+1) , \".This means that the agent has been reset (epoch > 1) or started (epoch = 1)\")\n","    loss = 0.0\n","\n","    if bigReset == True:\n","      print(\"We will BIG reset to the last valid new intersection we found: \", [listOfIntersections[-1][0],listOfIntersections[-1][1]])\n","      start_x = listOfIntersections[-1][0] # This is initialised  at [[1,1]]\n","      start_y = listOfIntersections[-1][1]\n","    else: \n","      print(\"We will reset to the last valid intersection we crossed unless resumingTraining was True when calling the function qtrain: \", [lastIntersection[0],lastIntersection[1]])\n","      start_x = lastIntersection[0] \n","      start_y = lastIntersection[1]\n","\n","    if resumingTraining == True:\n","      resumingTraining = False\n","      start_x = last_state_coordinates[0] \n","      start_y = last_state_coordinates[1]\n","    \n","    if TrainingFromStart == True:\n","      TrainingFromStart = False\n","      start_x = 1.0\n","      start_y = 1.0\n","\n","    game_over = False\n","\n","    resetAgent = False\n","\n","    # get initial state\n","    state = get_state(start_x, start_y)\n","    \n","    n_episodes = 0\n","\n","    while not game_over and resetAgent == False:\n","      prev_state = state\n","      prev_state_x = int(prev_state[0][-2])\n","      prev_state_y = int(prev_state[0][-1])\n","      last_state_coordinates = [state[0][-2],state[0][-1]]\n","      # Get next action\n","      if np.random.rand() < epsilon:\n","        action = float(random.choice(actions_list))\n","        random_move = \"random\"\n","      else:\n","        action = float(np.argmax(predict(updateModel, prev_state)))\n","        random_move = \"deterministic\"\n","\n","      if checkIfIntersection(state) == True:\n","        lastIntersection = [state[0][-2], state[0][-1], float(action)]\n","        print(\"We reached a new intersection: [x,y,action] = \", lastIntersection)\n","      elif [state[0][-2], state[0][-1]] == [1,1]: # Take into account the first try from [1,1] as well\n","        lastIntersection = [state[0][-2], state[0][-1], float(action)]\n","        print(\"We reached a new intersection: [x,y,action] = \", lastIntersection)\n","\n","\n","      # Apply action, get reward and new envstate\n","      new_state, reward, game_won, resetAgent = act(state, action)\n","\n","      new_state_x = int(new_state[0][-2])\n","      new_state_y = int(new_state[0][-1])\n","\n","       # Keep track of the trajectory:\n","      if [new_state[0][-2], new_state[0][-1]] not in visited:\n","        visited.append([new_state[0][-2], new_state[0][-1]])\n","\n","      if [new_state[0][-2], new_state[0][-1]] in BadStates:\n","        SetBadIntersectionMoves.append(lastIntersection)\n","\n","      if isBadIntersection(lastIntersection[0], lastIntersection[1]) == True:\n","        if [lastIntersection[0], lastIntersection[1]] not in BadStates:\n","          BadStates.append([lastIntersection[0], lastIntersection[1]])\n","        if [lastIntersection[0], lastIntersection[1]] in listOfIntersections:\n","          listOfIntersections.remove([lastIntersection[0], lastIntersection[1]])\n","        bigReset = True\n","        \n","      # Checkpoint\n","      print([prev_state[0][-2], prev_state[0][-1]], \" -> \", random_move , \" \", actions_dict[int(action)], \" -> \", [new_state[0][-2], new_state[0][-1]])\n","\n","      if game_won == True:\n","        game_over = True\n","        print(\"Game over. We reached the (199,199) tile.\")\n","        n_episodes += 1\n","\n","        episode = [state, action, reward, new_state, game_won]\n","        print(\"The last episode was: \", state,\"(state), \", action, \"(action), \", reward, \"(reward), \", new_state, \"(new_state), \", game_won, \"(game_won).\")\n","        memory.append(episode)\n","        \n","        if len(memory) > max_memory:\n","          del memory[0]\n","        inputs, targets = get_data(data_size, memory, num_actions, updateModel, targetModel, discount )\n","        h = updateModel.fit( # we update the model based on the targets\n","          inputs,\n","          targets,\n","          epochs=10,\n","          batch_size=25,\n","          verbose=0,\n","        )\n","        loss = updateModel.evaluate(inputs, targets, verbose=0)\n","        state = new_state\n","       \n","        # Save the results\n","        updateModel.save_weights('updateModelDQNwinner.h5')\n","        targetModel.save_weights('targetModelDQNwinner.h5')\n","        model_update_counter += 1\n","        with open(\"VisitedTilesWinner\", \"w\") as fp:\n","          json.dump(visited, fp)\n","        with open(\"SetBadIntersectionMovesWinner\", \"w\") as fp:\n","          json.dump(SetBadIntersectionMoves, fp)\n","        with open(\"BadStatesWinner\", \"w\") as fp:\n","          json.dump(BadStates, fp)\n","        with open(\"listOfIntersectionsWinner\", \"w\") as fp:\n","          json.dump(listOfIntersections, fp)\n","        with open(\"lastIntersectionWinner\", \"w\") as fp:\n","          json.dump(lastIntersection, fp)\n","        break\n","\n","      else:\n","        game_over = False\n","        \n","        n_episodes += 1\n","\n","        # Store episode for experience\n","        episode = [state, action, reward, new_state, game_won]\n","        memory.append(episode)\n","        \n","        if len(memory) > max_memory:\n","          del memory[0]\n","        # Train neural network model\n","        inputs, targets = get_data(data_size, memory, num_actions, updateModel, targetModel, discount)\n","        h = updateModel.fit(\n","          inputs,\n","          targets,\n","          epochs=10,\n","          batch_size=25,\n","          verbose=0,\n","        )\n","        loss = updateModel.evaluate(inputs, targets, verbose=0)\n","        state = new_state \n","\n","        last_state_coordinates = [new_state[0][-2],new_state[0][-1]]\n","\n","      model_update_counter += 1\n","\n","      # Update the network\n","      if (model_update_counter % 50) == 0:\n","        print(\"We reached \", model_update_counter, \" tries, let's update the target network and continue\")\n","        targetModel.set_weights(updateModel.get_weights()) # use that to copy the update's model weights to the target model\n","\n","      # Save the results regularly\n","      if (model_update_counter % 1000) == 0:\n","        print(\"We reached \", model_update_counter, \" tries, let's save the results and continue\")\n","        updateModel.save_weights('updateModelDQN.h5')\n","        targetModel.save_weights('targetModelDQN.h5')\n","        with open(\"VisitedTiles\", \"w\") as fp:\n","          json.dump(visited, fp)\n","        with open(\"SetBadIntersectionMoves\", \"w\") as fp:\n","          json.dump(SetBadIntersectionMoves, fp)\n","        with open(\"BadStates\", \"w\") as fp:\n","          json.dump(BadStates, fp)\n","        with open(\"listOfIntersections\", \"w\") as fp:\n","          json.dump(listOfIntersections, fp)\n","        with open(\"lastIntersection\", \"w\") as fp:\n","          json.dump(lastIntersection, fp)\n","        with open(\"last_state_coordinates\", \"w\") as fp:\n","          json.dump(last_state_coordinates, fp)\n","        \n","    \n","    dt = datetime.datetime.now() - start_time\n","    t = format_time(dt.total_seconds())\n","    template = \"Epoch: {:03d}/{:d} | Episodes: {:d} | time: {}\"\n","    print(template.format(epoch, n_epoch-1, n_episodes, t))\n","\n","\n","\n","\n","# This is a small utility for printing readable time strings:\n","def format_time(seconds):\n","    if seconds < 400:\n","        s = float(seconds)\n","        return \"%.1f seconds\" % (s,)\n","    elif seconds < 4000:\n","        m = seconds / 60.0\n","        return \"%.2f minutes\" % (m,)\n","    else:\n","        h = seconds / 3600.0\n","        return \"%.2f hours\" % (h,)"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"U0L-chtlFuIK","executionInfo":{"status":"ok","timestamp":1654765988119,"user_tz":-180,"elapsed":4,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[],"source":["# We are going to need a function that gives out the immediate rewards \n","# according to the (state, action) pair \n","\n","# We will encode the actions as follows:\n","LEFT = 0\n","UP = 1\n","RIGHT = 2\n","DOWN = 3\n","REMAIN = 4 \n","\n","# Actions dictionary\n","actions_dict = {\n","    LEFT: 'left',\n","    UP: 'up',\n","    RIGHT: 'right',\n","    DOWN: 'down' ,\n","    REMAIN: 'remain'\n","}\n","\n","num_actions = len(actions_dict) \n","actions_list= [0,1,2,3,4]\n","\n","# Exploration factor in order to pick random moves as well during training.\n","epsilon = 0.1"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"CpCCs52Ac7L-","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1654765988643,"user_tz":-180,"elapsed":27,"user":{"displayName":"konth konth","userId":"04638664992708773001"}},"outputId":"9dabdd12-2cb5-4cb9-ef36-4cf24e3beac6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nwith open(\"VisitedTiles\", \"r\") as fp:\\n  visited = json.load(fp)\\nwith open(\"last_state_coordinates\", \"r\") as fp:\\n  last_state_coordinates = json.load(fp)\\n\\nupdateModel.load_weights(\"updateModelDQN.h5\")\\ntargetModel.load_weights(\"targetModelDQN.h5\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":58}],"source":["target = (199, 199) # Set the target tile as you wish\n","\n","max_memory = 1000 # The maximum amount of tuples in the experience buffer\n","discount = 0.999 # The discount in the Bellman equation. Alter it accordingly\n","\n","memory = list() # our memory for the moves we have made\n","\n","SetBadIntersectionMoves = [[1.0 , 1.0 , 1.0]] # Initialise the set correctly\n","BadStates = list()\n","visited = list()\n","listOfIntersections = list()\n","lastIntersection = [1.0,1.0]\n","last_state_coordinates = list()\n","\n","\n","# Load previous models (optional). If you don't want to load weights then simply run the qtrain by commenting the two load_weigths commands below first \n","updateModel = build_model(lr = 0.001)\n","targetModel = build_model(lr = 0.001)\n","\n","targetModel.set_weights(updateModel.get_weights()) # It is better if you have both models the same from the start.\n","\n","\n","\n","# Comment or uncomment this section depending on whether you read previous files from your hard drive or want to start fresh\n","'''\n","with open(\"SetBadIntersectionMoves\", \"r\") as fp:\n","  SetBadIntersectionMoves = json.load(fp)\n","with open(\"BadStates\", \"r\") as fp:\n","  BadStates = json.load(fp)\n","with open(\"listOfIntersections\", \"r\") as fp:\n","  listOfIntersections = json.load(fp)\n","with open(\"lastIntersection\", \"r\") as fp:\n","  lastIntersection = json.load(fp)\n","'''\n","\n","'''\n","with open(\"VisitedTiles\", \"r\") as fp:\n","  visited = json.load(fp)\n","with open(\"last_state_coordinates\", \"r\") as fp:\n","  last_state_coordinates = json.load(fp)\n","\n","updateModel.load_weights(\"updateModelDQN.h5\")\n","targetModel.load_weights(\"targetModelDQN.h5\")\n","'''"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PFTCfvWwS04m","outputId":"e3ef599d-7b6b-4e9a-802c-3e4c2c3e5015","executionInfo":{"status":"error","timestamp":1654766094904,"user_tz":-180,"elapsed":105628,"user":{"displayName":"konth konth","userId":"04638664992708773001"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["We are now in epoch # 1 .This means that the agent has been reset (epoch > 1) or started (epoch = 1)\n","We will reset to the last valid intersection we crossed unless resumingTraining was True when calling the function qtrain:  [1.0, 1.0]\n","We reached a new intersection: [x,y,action] =  [1.0, 1.0, 1.0]\n","[1.0, 1.0]  ->  deterministic   up  ->  [1.0, 1.0]\n","Epoch: 000/99999 | Episodes: 1 | time: 1.0 seconds\n","We are now in epoch # 2 .This means that the agent has been reset (epoch > 1) or started (epoch = 1)\n","We will reset to the last valid intersection we crossed unless resumingTraining was True when calling the function qtrain:  [1.0, 1.0]\n","We reached a new intersection: [x,y,action] =  [1.0, 1.0, 4.0]\n","[1.0, 1.0]  ->  deterministic   remain  ->  [1.0, 1.0]\n","We reached a new intersection: [x,y,action] =  [1.0, 1.0, 4.0]\n","[1.0, 1.0]  ->  deterministic   remain  ->  [1.0, 1.0]\n","We reached a new intersection: [x,y,action] =  [1.0, 1.0, 0.0]\n","[1.0, 1.0]  ->  deterministic   left  ->  [1.0, 1.0]\n","We reached a new intersection: [x,y,action] =  [1.0, 1.0, 0.0]\n","[1.0, 1.0]  ->  deterministic   left  ->  [1.0, 1.0]\n","We reached a new intersection: [x,y,action] =  [1.0, 1.0, 3.0]\n","[1.0, 1.0]  ->  deterministic   down  ->  [2.0, 1.0]\n","[2.0, 1.0]  ->  deterministic   down  ->  [3.0, 1.0]\n","[3.0, 1.0]  ->  deterministic   down  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   right  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   down  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   down  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   down  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   remain  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  random   up  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   remain  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   remain  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   remain  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   remain  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  random   left  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   remain  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   remain  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   right  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  deterministic   down  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  random   left  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   down  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   up  ->  [4.0, 1.0]\n","[4.0, 1.0]  ->  random   down  ->  [5.0, 1.0]\n","[5.0, 1.0]  ->  deterministic   up  ->  [4.0, 1.0]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-fedade64f674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  epsilon, n_epoch, max_memory, data_size, discount, num_actions, **opt):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mqtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresumingTraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mTrainingFromStart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-56-70fe76354126>\u001b[0m in \u001b[0;36mqtrain\u001b[0;34m(epsilon, n_epoch, max_memory, data_size, discount, num_actions, resumingTraining, TrainingFromStart, **opt)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mrandom_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mrandom_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"deterministic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-e7ab71fffd13>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This will be used to return the output of the neural network:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3316\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#  epsilon, n_epoch, max_memory, data_size, discount, num_actions, **opt):\n","\n","qtrain(epsilon, 100000, 1000 , 500, discount, num_actions, resumingTraining = False,   TrainingFromStart = True)"]},{"cell_type":"code","source":["# After the initial training phase has reached the (199,199) tile retrain until sufficient results have been produced.\n","\n","for i in range(1000):\n","  target = (199, 199) \n"," \n","  epsilon = 0.1\n","\n","  discount = 0.999 \n","\n","  SetBadIntersectionMoves = [[1.0 , 1.0 , 1.0]]\n","  BadStates = list()\n","  visited = list()\n","  listOfIntersections = list()\n","  lastIntersection = [1.0,1.0]\n","  last_state_coordinates = list()\n","\n","  # Load previous models (optional). If you don't want to load weights then simply run the qtrain by commenting the two load_weigths commands below first \n","  updateModel = build_model(lr = 0.001)\n","  targetModel = build_model(lr = 0.001)\n","\n","  updateModel.load_weights(\"updateModelDQNWinner.h5\")\n","  targetModel.load_weights(\"targetModelDQNWinner.h5\")\n","\n","  \n","  # Load the previous results to facilitate training\n","  with open(\"VisitedTilesWinner\", \"r\") as fp: \n","    visited = json.load(fp)\n","\n","  with open(\"SetBadIntersectionMovesWinner\", \"r\") as fp:   \n","    SetBadIntersectionMoves = json.load(fp)\n","\n","  with open(\"BadStatesWinner\", \"r\") as fp:  \n","    BadStates = json.load(fp)\n","\n","  qtrain(epsilon, 100000, 1000 , 500, discount, num_actions, resumingTraining = False,   TrainingFromStart = True)"],"metadata":{"id":"teO_Y4Nsz3Xk"},"execution_count":null,"outputs":[]}]}